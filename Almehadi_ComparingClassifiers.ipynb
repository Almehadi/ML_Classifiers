{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Fake News Classifiers\n",
    "\n",
    "I wrote a longer explanation of the methodology and approach for detecting fake news using scikit-learn on DataCamp (and you can [find the notebook on my GitHub](https://github.com/kjam/random_hackery/blob/master/Attempting%20to%20detect%20fake%20news.ipynb)). I would start there if you are curious as to why I chose the data, what I learned about the models and so forth.\n",
    "\n",
    "In this notebook, I wanted to compare some of the features learned by each classifier to see if there was overlap or patterns in the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('fake_or_real_news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('label', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0        8476                       You Can Smell Hillary’s Fear   \n",
       "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4         875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text  \n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  \n",
       "4  It's primary day in New York and front-runners...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], y, test_size=0.33, random_state=53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "count_train = count_vectorizer.fit_transform(X_train)\n",
    "count_test = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training models\n",
    "\n",
    "Now I have my vectors and I can create my different classifiers. In my post I noted that there is definitely noise in the dataset, so we should expect to see that reflected in our features. Normally, I would spend some time cleaning the data, but this was a small proof of concept and investigation. I hoped merely that at least one model would be able to correct for the noise.\n",
    "\n",
    "I will compare the following models (and training data):\n",
    "\n",
    "- multinomialNB with counts (`sgd_count_clf`)\n",
    "- multinomialNB with tf-idf (`mn_tfidf_clf`)\n",
    "- passive aggressive with tf-idf (`pa_tfidf_clf`)\n",
    "- linear svc with tf-idf (`svc_tfidf_clf`)\n",
    "- linear sgd with tf-idf (`sgd_tfidf_clf`)\n",
    "\n",
    "For speed and clarity, I am primarily not doing parameter tuning, although this could be added as a step (perhaps in a scikit-learn Pipeline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn_count_clf = MultinomialNB(alpha=0.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.893\n"
     ]
    }
   ],
   "source": [
    "mn_count_clf.fit(count_train, y_train)\n",
    "pred = mn_count_clf.predict(count_test)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(\"accuracy:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn_tfidf_clf = MultinomialNB(alpha=0.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.898\n"
     ]
    }
   ],
   "source": [
    "mn_tfidf_clf.fit(tfidf_train, y_train)\n",
    "pred = mn_tfidf_clf.predict(tfidf_test)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(\"accuracy:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_tfidf_clf = PassiveAggressiveClassifier(n_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\almehadi.ali\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.936\n"
     ]
    }
   ],
   "source": [
    "pa_tfidf_clf.fit(tfidf_train, y_train)\n",
    "pred = pa_tfidf_clf.predict(tfidf_test)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(\"accuracy:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "svc_tfidf_clf = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.936\n"
     ]
    }
   ],
   "source": [
    "svc_tfidf_clf.fit(tfidf_train, y_train)\n",
    "pred = svc_tfidf_clf.predict(tfidf_test)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(\"accuracy:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_tfidf_clf = SGDClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\almehadi.ali\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "sgd_tfidf_clf.fit(tfidf_train, y_train)\n",
    "pred = sgd_tfidf_clf.predict(tfidf_test)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(\"accuracy:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_tfidf_clf.decision_function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn_count_clf.predict_proba?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Multinomial_Co' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-383a0a7f5010>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m for model, name in [ (Multinomial_Co, 'multinomial nb count'),\n\u001b[0m\u001b[0;32m      4\u001b[0m                      \u001b[1;33m(\u001b[0m\u001b[0mMultinomial_TFI\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'multinomial nb tfidf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                      \u001b[1;33m(\u001b[0m\u001b[0mSuportVC_tfidf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'svc tfidf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Multinomial_Co' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(0).clf()\n",
    "\n",
    "for model, name in [ (Multinomial_Co, 'multinomial nb count'),\n",
    "                     (Multinomial_TFI, 'multinomial nb tfidf'),\n",
    "                     (SuportVC_tfidf, 'svc tfidf'),\n",
    "                     (SuportVC_count, 'svc'),\n",
    "                    ]:\n",
    "    if 'count' in name:\n",
    "        pred = model.predict_proba(count_test)[:,1]\n",
    "    elif 'multinomial' in name:\n",
    "        pred = model.predict_proba(tfidf_test)[:,1]\n",
    "    else: \n",
    "        pred = model.decision_function(tfidf_test)\n",
    "    fpr, tpr, thresh = metrics.roc_curve(y_test.values, pred, pos_label='REAL')\n",
    "    plt.plot(fpr,tpr,label=\"{}\".format(name))\n",
    "\n",
    "plt.legend(loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introspecting models\n",
    "\n",
    "My main goal for this notebook is not to compare accuracy, but to compare features learned. To do so, we can use the method shown in this [very useful StackOverflow answer](https://stackoverflow.com/a/26980472) to show significant features in a binary classifier. I will use a modified version to return top features for each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FAKE': [(-4.998841923055006, '2016'),\n",
       "  (-4.320567465527726, 'october'),\n",
       "  (-3.9498769704981336, 'hillary'),\n",
       "  (-3.3291957511495864, 'share'),\n",
       "  (-2.9354203647936106, 'article'),\n",
       "  (-2.8235514980916614, 'november'),\n",
       "  (-2.494987356462026, 'print'),\n",
       "  (-2.4400387120907845, 'oct'),\n",
       "  (-2.4349762182259744, 'mosul'),\n",
       "  (-2.406066853191592, 'source')],\n",
       " 'REAL': [(2.204948440463152, 'islamic'),\n",
       "  (2.2095084101566718, 'rush'),\n",
       "  (2.2678771601967354, 'conservative'),\n",
       "  (2.3026504795105516, 'cruz'),\n",
       "  (2.3201488184667287, 'friday'),\n",
       "  (2.4045663377107336, 'marriage'),\n",
       "  (2.4277593848422265, 'gop'),\n",
       "  (2.495315020059606, 'says'),\n",
       "  (2.5979058805219535, 'tuesday'),\n",
       "  (4.751384220643409, 'said')]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def most_informative_feature_for_binary_classification(vectorizer, classifier, n=100):\n",
    "    \"\"\"\n",
    "    See: https://stackoverflow.com/a/26980472\n",
    "    \n",
    "    Identify most important features if given a vectorizer and binary classifier. Set n to the number\n",
    "    of weighted features you would like to show. (Note: current implementation merely prints and does not \n",
    "    return top classes.)\n",
    "    \n",
    "    Modified by @kjam to support a dict return.\n",
    "    \"\"\"\n",
    "\n",
    "    class_labels = classifier.classes_\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    topn_class1 = sorted(zip(classifier.coef_[0], feature_names))[:n]\n",
    "    topn_class2 = sorted(zip(classifier.coef_[0], feature_names))[-n:]\n",
    "\n",
    "    return {class_labels[0]: topn_class1,\n",
    "            class_labels[1]: topn_class2\n",
    "    }\n",
    "\n",
    "\n",
    "most_informative_feature_for_binary_classification(tfidf_vectorizer, pa_tfidf_clf, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [(mn_count_clf, count_vectorizer),\n",
    "               (mn_tfidf_clf, tfidf_vectorizer),\n",
    "               (pa_tfidf_clf, tfidf_vectorizer),\n",
    "               (svc_tfidf_clf, tfidf_vectorizer),\n",
    "               (sgd_tfidf_clf, tfidf_vectorizer)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for clf, vct in classifiers:\n",
    "    results[clf] = most_informative_feature_for_binary_classification(vct, clf, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True): {'FAKE': [(-16.067750538483136,\n",
       "    '0000'),\n",
       "   (-16.067750538483136, '000035'),\n",
       "   (-16.067750538483136, '0001'),\n",
       "   (-16.067750538483136, '0001pt'),\n",
       "   (-16.067750538483136, '000km'),\n",
       "   (-16.067750538483136, '0011'),\n",
       "   (-16.067750538483136, '006s'),\n",
       "   (-16.067750538483136, '007'),\n",
       "   (-16.067750538483136, '007s'),\n",
       "   (-16.067750538483136, '008s')],\n",
       "  'REAL': [(-5.675959082863306, 'republican'),\n",
       "   (-5.582298794347825, 'campaign'),\n",
       "   (-5.520542422049422, 'new'),\n",
       "   (-5.463370874939617, 'state'),\n",
       "   (-5.459162531269605, 'obama'),\n",
       "   (-5.429949870021241, 'president'),\n",
       "   (-5.403667459399097, 'people'),\n",
       "   (-4.929358535752954, 'clinton'),\n",
       "   (-4.541306857712, 'trump'),\n",
       "   (-4.424753408851144, 'said')]},\n",
       " MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True): {'FAKE': [(-12.641778440826338,\n",
       "    '0000'),\n",
       "   (-12.641778440826338, '000035'),\n",
       "   (-12.641778440826338, '0001'),\n",
       "   (-12.641778440826338, '0001pt'),\n",
       "   (-12.641778440826338, '000km'),\n",
       "   (-12.641778440826338, '0011'),\n",
       "   (-12.641778440826338, '006s'),\n",
       "   (-12.641778440826338, '007'),\n",
       "   (-12.641778440826338, '007s'),\n",
       "   (-12.641778440826338, '008s')],\n",
       "  'REAL': [(-6.452319082422527, 'cruz'),\n",
       "   (-6.452076515575875, 'state'),\n",
       "   (-6.397696648238072, 'republican'),\n",
       "   (-6.376343060363355, 'campaign'),\n",
       "   (-6.324397735392007, 'president'),\n",
       "   (-6.2546017970213645, 'sanders'),\n",
       "   (-6.144621899738043, 'obama'),\n",
       "   (-5.756817248152807, 'clinton'),\n",
       "   (-5.596085785733112, 'said'),\n",
       "   (-5.357523914504495, 'trump')]},\n",
       " PassiveAggressiveClassifier(C=1.0, average=False, class_weight=None,\n",
       "               fit_intercept=True, loss='hinge', max_iter=None, n_iter=50,\n",
       "               n_jobs=1, random_state=None, shuffle=True, tol=None,\n",
       "               verbose=0, warm_start=False): {'FAKE': [(-4.998841923055006,\n",
       "    '2016'),\n",
       "   (-4.320567465527726, 'october'),\n",
       "   (-3.9498769704981336, 'hillary'),\n",
       "   (-3.3291957511495864, 'share'),\n",
       "   (-2.9354203647936106, 'article'),\n",
       "   (-2.8235514980916614, 'november'),\n",
       "   (-2.494987356462026, 'print'),\n",
       "   (-2.4400387120907845, 'oct'),\n",
       "   (-2.4349762182259744, 'mosul'),\n",
       "   (-2.406066853191592, 'source')],\n",
       "  'REAL': [(2.204948440463152, 'islamic'),\n",
       "   (2.2095084101566718, 'rush'),\n",
       "   (2.2678771601967354, 'conservative'),\n",
       "   (2.3026504795105516, 'cruz'),\n",
       "   (2.3201488184667287, 'friday'),\n",
       "   (2.4045663377107336, 'marriage'),\n",
       "   (2.4277593848422265, 'gop'),\n",
       "   (2.495315020059606, 'says'),\n",
       "   (2.5979058805219535, 'tuesday'),\n",
       "   (4.751384220643409, 'said')]},\n",
       " LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "      intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "      multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "      verbose=0): {'FAKE': [(-2.573680306219974, '2016'),\n",
       "   (-2.5339769708504023, 'hillary'),\n",
       "   (-2.2832613153387964, 'october'),\n",
       "   (-1.7249561686063726, 'article'),\n",
       "   (-1.7001452171370022, 'november'),\n",
       "   (-1.6804827340025177, 'share'),\n",
       "   (-1.4612975105121488, 'election'),\n",
       "   (-1.3994871306572867, 'print'),\n",
       "   (-1.3618729038838728, 'war'),\n",
       "   (-1.3083269662603036, 'advertisement')],\n",
       "  'REAL': [(1.3417957530785076, 'friday'),\n",
       "   (1.348755985961213, 'monday'),\n",
       "   (1.3541846216663287, 'cruz'),\n",
       "   (1.3789176834788242, 'gop'),\n",
       "   (1.391985906749001, 'candidates'),\n",
       "   (1.4222354511822712, 'conservative'),\n",
       "   (1.4570592246685858, 'islamic'),\n",
       "   (1.5834301949542111, 'says'),\n",
       "   (1.6805116181764868, 'tuesday'),\n",
       "   (3.480213079126556, 'said')]},\n",
       " SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "        eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "        learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
       "        n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
       "        shuffle=True, tol=None, verbose=0, warm_start=False): {'FAKE': [(-3.9058833696675683,\n",
       "    '2016'),\n",
       "   (-3.7867289397371624, 'hillary'),\n",
       "   (-3.503509448472423, 'october'),\n",
       "   (-2.563997404205688, 'article'),\n",
       "   (-2.528809596844587, 'share'),\n",
       "   (-2.468594793439492, 'november'),\n",
       "   (-2.4020977610551735, 'print'),\n",
       "   (-2.350975971732242, 'election'),\n",
       "   (-2.090523907806039, 'advertisement'),\n",
       "   (-2.0007630300100625, 'establishment')],\n",
       "  'REAL': [(2.023017807586334, 'cruz'),\n",
       "   (2.0396262857232883, 'candidates'),\n",
       "   (2.076654061964815, 'friday'),\n",
       "   (2.156372886252091, 'monday'),\n",
       "   (2.217523933996199, 'conservative'),\n",
       "   (2.282681962511818, 'gop'),\n",
       "   (2.4462408419034536, 'says'),\n",
       "   (2.474274542399607, 'islamic'),\n",
       "   (2.7217989233698248, 'tuesday'),\n",
       "   (4.8296493596310635, 'said')]}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this is both a bit hard to read and compare. What I really want is to see these possibly with ranks and compare the tokens to one another. Let's transform the data to look better for what we are trying to measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparable_results = {'REAL': {}, 'FAKE': {}}\n",
    "for clf, data in results.items():\n",
    "    clf_name = clf.__class__.__name__\n",
    "    for label, features in data.items():\n",
    "        for rank, score_tuple in enumerate(features):\n",
    "            if score_tuple[1] in comparable_results[label]:\n",
    "                comparable_results[label][score_tuple[1]].append((rank + 1, clf_name))\n",
    "            else:\n",
    "                comparable_results[label][score_tuple[1]] = [(rank + 1, clf_name)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now these are a bit easier to compare and read:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0000': [(1, 'MultinomialNB'), (1, 'MultinomialNB')],\n",
       " '000035': [(2, 'MultinomialNB'), (2, 'MultinomialNB')],\n",
       " '0001': [(3, 'MultinomialNB'), (3, 'MultinomialNB')],\n",
       " '0001pt': [(4, 'MultinomialNB'), (4, 'MultinomialNB')],\n",
       " '000km': [(5, 'MultinomialNB'), (5, 'MultinomialNB')],\n",
       " '0011': [(6, 'MultinomialNB'), (6, 'MultinomialNB')],\n",
       " '006s': [(7, 'MultinomialNB'), (7, 'MultinomialNB')],\n",
       " '007': [(8, 'MultinomialNB'), (8, 'MultinomialNB')],\n",
       " '007s': [(9, 'MultinomialNB'), (9, 'MultinomialNB')],\n",
       " '008s': [(10, 'MultinomialNB'), (10, 'MultinomialNB')],\n",
       " '2016': [(1, 'PassiveAggressiveClassifier'),\n",
       "  (1, 'LinearSVC'),\n",
       "  (1, 'SGDClassifier')],\n",
       " 'october': [(2, 'PassiveAggressiveClassifier'),\n",
       "  (3, 'LinearSVC'),\n",
       "  (3, 'SGDClassifier')],\n",
       " 'hillary': [(3, 'PassiveAggressiveClassifier'),\n",
       "  (2, 'LinearSVC'),\n",
       "  (2, 'SGDClassifier')],\n",
       " 'share': [(4, 'PassiveAggressiveClassifier'),\n",
       "  (6, 'LinearSVC'),\n",
       "  (5, 'SGDClassifier')],\n",
       " 'article': [(5, 'PassiveAggressiveClassifier'),\n",
       "  (4, 'LinearSVC'),\n",
       "  (4, 'SGDClassifier')],\n",
       " 'november': [(6, 'PassiveAggressiveClassifier'),\n",
       "  (5, 'LinearSVC'),\n",
       "  (6, 'SGDClassifier')],\n",
       " 'print': [(7, 'PassiveAggressiveClassifier'),\n",
       "  (8, 'LinearSVC'),\n",
       "  (7, 'SGDClassifier')],\n",
       " 'oct': [(8, 'PassiveAggressiveClassifier')],\n",
       " 'mosul': [(9, 'PassiveAggressiveClassifier')],\n",
       " 'source': [(10, 'PassiveAggressiveClassifier')],\n",
       " 'election': [(7, 'LinearSVC'), (8, 'SGDClassifier')],\n",
       " 'war': [(9, 'LinearSVC')],\n",
       " 'advertisement': [(10, 'LinearSVC'), (9, 'SGDClassifier')],\n",
       " 'establishment': [(10, 'SGDClassifier')]}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparable_results['FAKE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I immediately noticed the multinomial models had picked up quite a bit of noise from the dataset. These models likely would have benefit from some preprocessing. I also noticed that *most* of the models had picked up what I would consider noise, such as `2016` and the words `print` and `share` (which are clearly scraping artifacts).\n",
    "\n",
    "Let's see if we can score the tokens by popularity and rank. I also wanted to add in a warning message in case I had overlap between my real and fake tokens. (This may be the case if you take a larger n-features from each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_results = {}\n",
    "for label, features in comparable_results.items():\n",
    "    for feature, ranks in features.items():\n",
    "        if feature in agg_results:\n",
    "            print(\"WARNING! DUPLICATE LABEL!!! {}\".format(feature))\n",
    "        agg_results[feature] = {\n",
    "            'label': label,\n",
    "            'agg_rank': np.mean([r[0] for r in ranks]),\n",
    "            'count': len(ranks)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can then put this into a dataframe, for easier transformations and viewing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = pd.DataFrame(agg_results).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg_rank</th>\n",
       "      <th>count</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>republican</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>campaign</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>obama</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           agg_rank count label\n",
       "republican        2     2  REAL\n",
       "campaign          3     2  REAL\n",
       "new               3     1  REAL\n",
       "state             3     2  REAL\n",
       "obama             6     2  REAL"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To investigate the top real and fake labels, I would advise to sort by count. Let's see my top 10 tokens for real and fake news ranked by the number of classifiers that used them as a top feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg_rank</th>\n",
       "      <th>count</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>said</th>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cruz</th>\n",
       "      <td>2.25</td>\n",
       "      <td>4</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tuesday</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>says</th>\n",
       "      <td>7.66667</td>\n",
       "      <td>3</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gop</th>\n",
       "      <td>5.66667</td>\n",
       "      <td>3</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>friday</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conservative</th>\n",
       "      <td>4.66667</td>\n",
       "      <td>3</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>islamic</th>\n",
       "      <td>5.33333</td>\n",
       "      <td>3</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>republican</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>campaign</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             agg_rank count label\n",
       "said              9.8     5  REAL\n",
       "cruz             2.25     4  REAL\n",
       "tuesday             9     3  REAL\n",
       "says          7.66667     3  REAL\n",
       "gop           5.66667     3  REAL\n",
       "friday              3     3  REAL\n",
       "conservative  4.66667     3  REAL\n",
       "islamic       5.33333     3  REAL\n",
       "republican          2     2  REAL\n",
       "campaign            3     2  REAL"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_df[comparison_df['label'] == 'REAL'].sort_values('count', ascending=0).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg_rank</th>\n",
       "      <th>count</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hillary</th>\n",
       "      <td>2.33333</td>\n",
       "      <td>3</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>print</th>\n",
       "      <td>7.33333</td>\n",
       "      <td>3</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>november</th>\n",
       "      <td>5.66667</td>\n",
       "      <td>3</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article</th>\n",
       "      <td>4.33333</td>\n",
       "      <td>3</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>share</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>october</th>\n",
       "      <td>2.66667</td>\n",
       "      <td>3</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>advertisement</th>\n",
       "      <td>9.5</td>\n",
       "      <td>2</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>election</th>\n",
       "      <td>7.5</td>\n",
       "      <td>2</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000035</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              agg_rank count label\n",
       "hillary        2.33333     3  FAKE\n",
       "2016                 1     3  FAKE\n",
       "print          7.33333     3  FAKE\n",
       "november       5.66667     3  FAKE\n",
       "article        4.33333     3  FAKE\n",
       "share                5     3  FAKE\n",
       "october        2.66667     3  FAKE\n",
       "advertisement      9.5     2  FAKE\n",
       "election           7.5     2  FAKE\n",
       "000035               2     2  FAKE"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_df[comparison_df['label'] == 'FAKE'].sort_values('count', ascending=0).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "As expected, the bag-of-words and TF-IDF vectors didn't do much to determine meaningful features to classify fake or real news. As outlined in my DataCamp post, this problem is a lot harder than simple text classification.\n",
    "\n",
    "That said, I did learn a few things. Namely, that linear models handle noise in this case better than the Naive Bayes multinomial classifier did. Also, finding a good dataset that has been scraped from the web and tagged for this problem would likely be a great help, and worth more of my time than parameter tuning on a clearly noisy and error prone dataset.\n",
    "\n",
    "If you spend some time researching and find anything interesting, feel free to share your findings and notes in the comments or you can always reach out on Twitter (I'm [@kjam](https://twitter.com/kjam)).\n",
    "\n",
    "I hope you had some fun exploring a new NLP dataset with me!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix A: Top features\n",
    "\n",
    "Once I realized the Naive Bayes classifiers had identified many noisy tokens in alphabetical order as top fake news classifiers, I decided to see just how many \"top features\" the model had. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20161 (-13.669855265684765, '00000031')\n"
     ]
    }
   ],
   "source": [
    "feature_names = count_vectorizer.get_feature_names()\n",
    "for idx, ftr_weight in enumerate(sorted(zip(mn_count_clf.coef_[0], feature_names))):\n",
    "    if ftr_weight[0] <= -16.067750538483136:\n",
    "        continue\n",
    "    print(idx, ftr_weight)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
